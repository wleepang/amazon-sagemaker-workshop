{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon SageMaker with Public Datasets\n",
    "\n",
    "__*Clustering Gene Variants into Geographic Populations*__\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker allows you to bring powerful machine learning workflows to data that is already in the cloud.  In this example, we will do just that - combining Amazon SageMaker with data from the [1000 Genomes Project] which is hosted by AWS as a [public dataset].  Specifically, we will perform unsupervised learning using Amazon SageMaker's KMeans algorithm to see if we can predict the geographic population for a set of single nucleotide polymorphisms.\n",
    "\n",
    "Single nucleotide polymorphisms or SNPs (pronounced \"snips\") are single base-pair changes to DNA.  DNA is a long chain molecule that is used to store the \"source code\" for all living organisms and is \"read\" as a sequence of four nucleotides: A, T, C, and G.  A single letter is called a \"base\".  SNPs occur when one of these bases in the sequence changes due to environmental causes or random replication errors during cell division in germ cells (eggs and sperm).  Sometimes these changes are harmless, and sometimes they can cause serious diseases.\n",
    "\n",
    "Here we are going to cluster high frequency SNPs found on Chromosome 6\n",
    "\n",
    "### Attribution\n",
    "This notebook is based on work previously described by [Databricks using Spark][databricks blog]\n",
    "\n",
    "[1000 Genomes Project]: https://aws.amazon.com/1000genomes/\n",
    "[public dataset]: https://aws.amazon.com/public-datasets/\n",
    "[databricks blog]: https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "> This notebook was created and tested on an `ml.m4.2xlarge` notebook instance\n",
    "\n",
    "Let's start by:\n",
    "\n",
    "1. Downloading the data we need from S3\n",
    "1. Installing some utility packages for processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "We can get variant call data (which describes SNPs) from the publicly hosted 1000 Genomes dataset on AWS.  We are need the \"\\*.vcf\" file corresponding to Chromosome 6 from the 20130502 release of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 ls --human-readable s3://1000genomes/release/20130502/ | grep chr6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for Chromosome 6 is nearly 1GB in size.  For the purpose of this exercise and to be conservative of space (the scratch area for sagemaker notebooks only have about 5GB of space) we are going to use a sub-sample of the data.  To generate that, we can use `tabix` a bioinformatics command line utility found in the [htslib] set of tools.\n",
    "\n",
    "[htslib]: https://github.com/samtools/htslib\n",
    "\n",
    "A version of `tabix` compiled for Linux was created and published to S3.  We'll download that into our SageMaker environment and use it to sample the Chromosome 6 VCF file __*directly on S3*__ and create a data file we can use here for model training.  Here we've reduced the data to entries found between positions 1000000-1250000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 cp s3://pwyming-utils/htslib/tabix .\n",
    "\n",
    "chmod +x ./tabix\n",
    "\n",
    "./tabix -h \\\n",
    "    s3://1000genomes/release/20130502/ALL.chr6.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz \\\n",
    "    6:1000000-1250000 > 6-sample.vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab metadata - information about geographic locations of where sample sequences came from - to use as labels in our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "aws s3 cp s3://1000genomes/release/20130502/integrated_call_samples_v3.20130502.ALL.panel ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "To make exploring and processing the data a little easier, we'll use the `scikit-allel` package.  While this package does not come included with the SageMaker environment, it is easy to install.\n",
    "\n",
    "More information about it can be found at:\n",
    "http://scikit-allel.readthedocs.io/en/latest/index.html\n",
    "\n",
    "It has good utilities for reading VCF files\n",
    "http://alimanfoo.github.io/2017/06/14/read-vcf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y scikit-allel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily read in our sampled VCF file for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import allel\n",
    "\n",
    "callset = allel.read_vcf('6-sample.vcf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns a Python dictionary with keys that represent parts of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many variants and samples are in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Samples: {samples}, Variants: {variants}\".format(\n",
    "    variants=len(callset['calldata/GT']), \n",
    "    samples=len(callset['samples']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be up to 6 alternate alleles for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(callset['calldata/GT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferred a different way using the `GenotypeArray` object ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gt = allel.GenotypeArray(callset['calldata/GT'])\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_gts = np.unique(gt.to_gt())\n",
    "unq_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unq_gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GenotypeArray` also tells us that there are no missing calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.count_missing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variants are a combination of SNPs, InDels, and Copy Number variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(callset['variants/ALT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Modeling Problem\n",
    "\n",
    "### Feature selection\n",
    "For this modeling exercise, we are going to use the variant \"ID\" - combination of the chromosome position, the \"reference\" allele, and the \"alternative\" allele as features for K-Means clustering.  To simplify things, lets just focus on SNPs that are bi-allelic variants.  To do this, we filter for entries where there is only 1 nucleotide in the REF and ALT lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8090 single nucleotide reference alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REF = callset['variants/REF']\n",
    "REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "is_1bp_ref = np.array(list(map(lambda r: len(r) == 1, REF)), dtype=bool)\n",
    "sum(is_1bp_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8147 single nucleotide alternative alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the default import considers only 3 allele alternatives\n",
    "ALT = callset['variants/ALT']\n",
    "ALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "is_1bp_alt = np.array(list(map(lambda a: len(a[0]) == 1 and not any(a[1:]), ALT)), dtype=bool)\n",
    "sum(is_1bp_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intersection of the above yields 7946 SNP variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_snp_var = is_1bp_ref & is_1bp_alt\n",
    "sum(is_snp_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce the feature set further by only considering variants with allele frequencies >= 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a count of alleles for each variant\n",
    "ac = gt.count_alleles()\n",
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac[ac[:,1].argsort()[::-1]][:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# since we're only looking for only bi-allelic SNPs, we're only concerned with the first alternative allele\n",
    "is_hifreq_snp_var = is_snp_var & (ac[:, 1] >= 30)\n",
    "sum(is_hifreq_snp_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now down to ~1700 features, which is certainly more manageable than the ~8300 we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "\n",
    "Machine learning algorithms work best on numerical data.\n",
    "\n",
    "We can convert the genotypes into integer values easily using bit-packing provided by the `GenotypeArray.to_packed` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = allel.GenotypeArray([\n",
    "    [[0,0], [0,1], [1,0], [1,1]]\n",
    "])\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.to_packed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that a values of 1 and 16 are effectively equivalent - they correspond to the same genotype: heterozygous for the alt allele.  Where 17 is homozygous for the alt allele.  We can recode these to:\n",
    "* 16 --> 1\n",
    "* 17 --> 2\n",
    "\n",
    "We'll apply this transformation to our GenoTypeArray, which we'll use as our data for training, and apply the filter for only high frequency SNPs that we generated above.  We also want the data entries to list samples along the rows and variants along the columns, so we'll use the transpose of the coded GenoTypeArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_coded = gt.to_packed()[is_hifreq_snp_var,:]\n",
    "gt_coded[gt_coded == 16] = 1\n",
    "gt_coded[gt_coded == 17] = 2\n",
    "gt_coded.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `CHROM`, `POS`, `REF`, and `ALT` to create variant feature IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    '{}-{}-{}-{}'.format(c, p, r, a[0])\n",
    "    for c, p, r, a in zip(\n",
    "        callset['variants/CHROM'], \n",
    "        callset['variants/POS'], \n",
    "        callset['variants/REF'], \n",
    "        callset['variants/ALT']\n",
    "    )]\n",
    "features = np.array(features)[is_hifreq_snp_var]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the panel metadata to get class labels - the geographic location that each sample originated from.  To simplify the clustering, we'll only focus on the following populations:\n",
    "\n",
    "* GBR: British from England and Scotland\n",
    "* ASW: African Ancestry in Southwest US\n",
    "* CHB: Han Chinese in Bejing, China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "classes = pd.read_table('integrated_call_samples_v3.20130502.ALL.panel', usecols=['sample', 'pop'])\n",
    "classes = classes[classes['pop'].isin(['GBR', 'ASW', 'CHB'])].copy()\n",
    "classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "classes.groupby('pop').count().reset_index().plot.bar('pop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of samples across these three populations looks reasonable.\n",
    "\n",
    "\n",
    "Let's now create the data frame to feed into model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = classes.merge(\n",
    "    pd.concat((\n",
    "        pd.Series(callset['samples'], name='sample'),\n",
    "        pd.DataFrame(gt_coded.transpose(), columns=features)),\n",
    "        axis=1),\n",
    "    on='sample'\n",
    ")\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure 3 populations to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pop'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "This dataset is small, only 255 observations, but should give an idea of how to use the built-in KMeans algorithm.\n",
    "\n",
    "Let's use an 80/20 ratio for the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "\n",
    "train_data = data.sample(frac=.8, random_state=1024)\n",
    "test_data = data[~data['sample'].isin(train_data['sample'])].copy()\n",
    "\n",
    "print('Observations')\n",
    "print(f'training: {train_data.shape[0]}')\n",
    "print(f'test: {test_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train_data[['sample', 'pop']].copy().set_index('sample')\n",
    "train_labels['pop'] = pd.Categorical(train_labels['pop'])\n",
    "train_data = train_data.copy().drop(columns='pop').set_index('sample')\n",
    "\n",
    "test_labels = test_data[['sample', 'pop']].copy().set_index('sample')\n",
    "test_labels['pop'] = pd.Categorical(test_labels['pop'])\n",
    "test_data = test_data.copy().drop(columns='pop').set_index('sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import KMeans, get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "bucket = 'pwyming-sagemaker-us-west-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 's3://{}/sagemaker/genome-kmeans/data'.format(bucket)\n",
    "output_location = 's3://{}/sagemaker/genome-kmeans/output'.format(bucket)\n",
    "\n",
    "print('training data will be uploaded to: {}'.format(data_location))\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))\n",
    "\n",
    "kmeans = KMeans(role=role,\n",
    "                train_instance_count=2,\n",
    "                train_instance_type='ml.c4.8xlarge',\n",
    "                output_path=output_location,\n",
    "                k=3,\n",
    "                data_location=data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans.fit(kmeans.record_set(np.float32(train_data.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Endpoint Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = kmeans_predictor.predict(np.float32(train_data))\n",
    "clusters = np.int0([r.label['closest_cluster'].float32_tensor.values[0] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these predicted clusters map to the real classes\n",
    "\n",
    "First, how well did the training set cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_labels['pop'], columns=clusters, colnames=['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this cross tabulation:\n",
    "* Cluster 0 primarily identifies CHB\n",
    "* Cluster 1 primarily identifies GBR\n",
    "* Cluster 2 exclusively identifies ASW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering results are roughly the same on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kmeans_predictor.predict(np.float32(test_data))\n",
    "clusters = np.int0([r.label['closest_cluster'].float32_tensor.values[0] for r in result])\n",
    "pd.crosstab(test_labels['pop'], columns=clusters, colnames=['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom Line\n",
    "\n",
    "The mixture of populations in the clusters may be interpretted as individuals with mixed ancestry.  Also, the clustering could be improved further if there was additional dimensionality reduction (e.g. via PCA), more samples, or both.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Delete the Endpoint\n",
    "If you're ready to be done with this notebook, make sure run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(kmeans_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker.Session().delete_endpoint(kmeans_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
